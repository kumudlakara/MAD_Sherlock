{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    #print(\"Number of data points: \", len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_annotations():\n",
    "    visual_news_data = json.load(open(\"../../datasets/visualnews/origin/data.json\"))\n",
    "    visual_news_data_mapping = {ann[\"id\"]: ann for ann in visual_news_data}\n",
    "\n",
    "    test_data = json.load(open(\"../../news_clippings/news_clippings/data/merged_balanced/test.json\"))\n",
    "    annotations = test_data[\"annotations\"]\n",
    "    return annotations\n",
    "test_data_annotations = get_test_data_annotations()\n",
    "test_data_annotations = test_data_annotations[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unsures(data):\n",
    "    \"\"\"\n",
    "    In instances where one of the models does not have any output, consider the output of the other model to be the truth\n",
    "    \"\"\"\n",
    "    num_unsures = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            num_unsures += 1\n",
    "            if data[i]['output']['model_0'] == \"\" and data[i]['output']['model_1'] != \"\":\n",
    "                if \"YES\" in data[i]['output']['model_1'] or \"Yes\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_1'] or \"No\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = False\n",
    "            elif data[i]['output']['model_0'] != \"\" and data[i]['output']['model_1'] == \"\":\n",
    "                if \"YES\" in data[i]['output']['model_0'] or \"Yes\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_0'] or \"No\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = False\n",
    "    print(\"Num unsures: \", num_unsures)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_disagreements(data):\n",
    "    \"\"\"\n",
    "    Function to check when models actually disagree and when they are just unsure\n",
    "    \"\"\"\n",
    "    disagreements = 0\n",
    "    for i in range(len(data)):\n",
    "        m0 = data[i]['output']['model_0']\n",
    "        m1 = data[i]['output']['model_1']\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            #check if models disagree\n",
    "            if (\"YES\" in m0 or \"Yes\" in m0) and (\"NO\" in m1 or \"No\" in m1 or \"Unsure\" in m1):\n",
    "                disagreements += 1\n",
    "            elif (\"NO\" in m0 or \"No\" in m0) and (\"YES\" in m1 or \"Yes\" in m1 or \"Unsure\" in m1):\n",
    "                disagreements += 1\n",
    "            elif \"Unsure\" in m0 and (\"YES\" in m1 or \"NO\" in m1 or \"Yes\" in m1 or \"No\" in m1):\n",
    "                disagreements += 1\n",
    "    print(\"Num disagreements: \", disagreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, annotations):\n",
    "    num_correct = 0\n",
    "    data = fix_unsures(data)\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            incorrect_idx.append(i)\n",
    "    print(\"Num incorrects: \", len(incorrect_idx))\n",
    "    return num_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_idx(data, annotations):\n",
    "    num_correct = 0\n",
    "    data = fix_unsures(data)\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) != annotations[i]['falsified']:\n",
    "            incorrect_idx.append(i)\n",
    "    return incorrect_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_without_unsures(data, annotations):\n",
    "    num_correct, num_unsures = 0,0\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['falsified'] == 'Unsure':\n",
    "            num_unsures += 1\n",
    "        elif bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "    return num_correct/(len(data) - num_unsures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(data, annotations):\n",
    "    num_tp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == True:\n",
    "            num_tp += 1\n",
    "    return num_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positives(data, annotations):\n",
    "    num_fp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == False and bool(data[i]['falsified']) == True:\n",
    "            num_fp += 1\n",
    "    return num_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negatives(data, annotations):\n",
    "    num_fn = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == False:\n",
    "            num_fn += 1\n",
    "    return num_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_and_recall(data, annotations):\n",
    "    tp,fp,fn = true_positives(data, annotations), false_positives(data, annotations), false_negatives(data, annotations)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no web access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  4\n",
      "Num unsures:  675\n",
      "Num incorrects:  428\n",
      "Accuracy: 0.5720\n",
      "Precision: 0.5448\n",
      "Recall: 0.8640\n",
      "Accuracy without unsures: 0.7515\n"
     ]
    }
   ],
   "source": [
    "no_web_file = \"../results/results_no_web_access.json\"\n",
    "result_data = get_results(no_web_file)\n",
    "result_data = result_data[:1000]\n",
    "num_disagreements(result_data)\n",
    "precision, recall = get_precision_and_recall(result_data, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data, test_data_annotations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with web access (only when model unsure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  16\n",
      "Num unsures:  329\n",
      "Accuracy: 0.5960\n",
      "Precision: 0.5804\n",
      "Recall: 0.6860\n",
      "Accuracy without unsures: 0.6433\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_web_access_no_initial_context.json\"\n",
    "result_data2 = get_results(web_access_file)\n",
    "num_disagreements(result_data2)\n",
    "precision, recall = get_precision_and_recall(result_data2, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data2, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data2, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  7\n",
      "Num unsures:  9\n",
      "Accuracy: 0.8580\n",
      "Precision: 0.8266\n",
      "Recall: 0.9060\n",
      "Accuracy without unsures: 0.8587\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_initial_context.json\"\n",
    "result_data3 = get_results(web_access_file)\n",
    "num_disagreements(result_data3)\n",
    "precision, recall = get_precision_and_recall(result_data3, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data3, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data3, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with disambiguation queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  5\n",
      "Num unsures:  12\n",
      "Accuracy: 0.7730\n",
      "Precision: 0.7468\n",
      "Recall: 0.8260\n",
      "Accuracy without unsures: 0.7786\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_with_disambiguation.json\"\n",
    "result_data4 = get_results(web_access_file)\n",
    "num_disagreements(result_data4)\n",
    "precision, recall = get_precision_and_recall(result_data4, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data4, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data4, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with opposite stances and disambiguation queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_access_file = \"../scripts/temp_final_disamb.json\"\n",
    "result_data5 = get_results(web_access_file)\n",
    "num_disagreements(result_data5)\n",
    "precision, recall = get_precision_and_recall(result_data5, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data5, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data5, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "Num disagreements:  1\n",
      "Num unsures:  6\n",
      "Accuracy: 0.7352\n",
      "Precision: 0.7153\n",
      "Recall: 0.7778\n",
      "Accuracy without unsures: 0.7368\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../scripts/disamb_res_updated_1.json\"\n",
    "result_data5 = get_results(web_access_file)\n",
    "print(len(result_data5))\n",
    "num_disagreements(result_data5)\n",
    "precision, recall = get_precision_and_recall(result_data5, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data5, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data5, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actor-skeptic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Precision: 0.8166\n",
      "Recall: 0.8640\n",
      "Accuracy without unsures: 0.8398\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/as_temp.json\"\n",
    "result_data6 = get_results(web_access_file)\n",
    "print(len(result_data6))\n",
    "#num_disagreements(result_data6)\n",
    "precision, recall = get_precision_and_recall(result_data6, test_data_annotations)\n",
    "#print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data6, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data6, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuned model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Num disagreements:  8\n",
      "Num unsures:  25\n",
      "Num incorrects:  182\n",
      "Accuracy: 0.8180\n",
      "Precision: 0.7449\n",
      "Recall: 0.9580\n",
      "Accuracy without unsures: 0.8192\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/finetuned_results.json\"\n",
    "result_data7 = get_results(web_access_file)\n",
    "print(len(result_data7))\n",
    "num_disagreements(result_data7)\n",
    "precision, recall = get_precision_and_recall(result_data7, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data7, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data7, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing where model fails (what kinds of examples the model fails on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils.data import get_data, show_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_summary(key):\n",
    "    with open(\"../utils/summaries.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_samples(incorrect_idx, res_data):\n",
    "    for i in incorrect_idx:\n",
    "        img, caption, _, annotation = get_data(i)\n",
    "        display(img)\n",
    "        show_data(i)\n",
    "        key = str(annotation['id'])+\"_\"+str(annotation['image_id'])\n",
    "        print(\"Associated summary: \", retrieve_summary(key))\n",
    "        print(\"Model_prediction: \", res_data[i]['falsified'])\n",
    "        print(\"Model arguments: \", res_data[i]['output'])\n",
    "        cont = input()\n",
    "        if \"exit\" == cont:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unsures:  39\n",
      "[2, 14, 20, 23, 31, 36, 44, 54, 62, 64, 74, 76, 78, 80, 86, 98, 110, 112, 114, 116, 117, 118, 120, 130, 136, 140, 142, 150, 152, 154, 158, 160, 164, 166, 167, 168, 170, 174, 175, 183, 184, 189, 190, 199, 205, 206, 218, 224, 229, 234, 235, 241, 242, 244, 245, 246, 254, 260, 267, 272, 274, 284, 286, 294, 296, 300, 304, 308, 316, 322, 324, 330, 334, 335, 338, 341, 342, 343, 348, 354, 360, 361, 362, 366, 371, 372, 375, 376, 380, 382, 384, 386, 398, 404, 408, 410, 412, 416, 418, 425, 426, 428, 432, 436, 438, 456, 464, 466, 470, 473, 480, 481, 490, 498, 502, 516, 520, 524, 528, 530, 532, 534, 535, 543, 552, 554, 556, 562, 566, 568, 574, 580, 582, 586, 590, 596, 600, 602, 608, 613, 614, 623, 626, 628, 636, 638, 640, 642, 644, 648, 650, 651, 652, 656, 666, 672, 676, 678, 684, 688, 693, 698, 700, 710, 712, 720, 721, 722, 724, 726, 728, 732, 734, 736, 738, 740, 744, 760, 762, 768, 771, 772, 774, 778, 786, 790, 794, 798, 802, 806, 810, 814, 816, 822, 824, 826, 833, 835, 836, 838, 842, 847, 854, 856, 858, 860, 872, 874, 884, 888, 890, 895, 896, 897, 900, 904, 914, 920, 922, 926, 928, 930, 932, 938, 940, 941, 942, 944, 951, 952, 954, 962, 972, 974, 978, 995]\n"
     ]
    }
   ],
   "source": [
    "incorrect_idx = get_incorrect_idx(result_data7, test_data_annotations)\n",
    "print(incorrect_idx)\n",
    "#failed_samples(incorrect_idx, result_data7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
