{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    #print(\"Number of data points: \", len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_annotations():\n",
    "    visual_news_data = json.load(open(\"../../datasets/visualnews/origin/data.json\"))\n",
    "    visual_news_data_mapping = {ann[\"id\"]: ann for ann in visual_news_data}\n",
    "\n",
    "    test_data = json.load(open(\"../../news_clippings/news_clippings/data/merged_balanced/test.json\"))\n",
    "    annotations = test_data[\"annotations\"]\n",
    "    return annotations\n",
    "test_data_annotations = get_test_data_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unsures(data):\n",
    "    \"\"\"\n",
    "    In instances where one of the models does not have any output, consider the output of the other model to be the truth\n",
    "    \"\"\"\n",
    "    num_unsures = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            num_unsures += 1\n",
    "            if data[i]['output']['model_0'] == \"\" and data[i]['output']['model_1'] != \"\":\n",
    "                if \"YES\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = False\n",
    "            elif data[i]['output']['model_0'] != \"\" and data[i]['output']['model_1'] == \"\":\n",
    "                if \"YES\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = False\n",
    "    print(\"Num unsures: \", num_unsures)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_disagreements(data):\n",
    "    \"\"\"\n",
    "    Function to check when models actually disagree and when they are just unsure\n",
    "    \"\"\"\n",
    "    disagreements = 0\n",
    "    for i in range(len(data)):\n",
    "        m0 = data[i]['output']['model_0']\n",
    "        m1 = data[i]['output']['model_1']\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            #check if models disagree\n",
    "            if \"YES\" in m0 and (\"NO\" in m1 or \"UNSURE\" in m1):\n",
    "                disagreements += 1\n",
    "            elif \"NO\" in m0 and (\"YES\" in m1 or \"UNSURE\" in m1):\n",
    "                disagreements += 1\n",
    "            elif \"UNSURE\" in m0 and (\"YES\" in m1 or \"NO\" in m1):\n",
    "                disagreements += 1\n",
    "    print(\"Num disagreements: \", disagreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, annotations):\n",
    "    num_correct = 0\n",
    "    data = fix_unsures(data)\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            incorrect_idx.append(i)\n",
    "    return num_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_idx(data, annotations):\n",
    "    num_correct = 0\n",
    "    data = fix_unsures(data)\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) != annotations[i]['falsified']:\n",
    "            incorrect_idx.append(i)\n",
    "    return incorrect_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_without_unsures(data, annotations):\n",
    "    num_correct, num_unsures = 0,0\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['falsified'] == 'Unsure':\n",
    "            num_unsures += 1\n",
    "        elif bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "    return num_correct/(len(data) - num_unsures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(data, annotations):\n",
    "    num_tp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == True:\n",
    "            num_tp += 1\n",
    "    return num_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positives(data, annotations):\n",
    "    num_fp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == False and bool(data[i]['falsified']) == True:\n",
    "            num_fp += 1\n",
    "    return num_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negatives(data, annotations):\n",
    "    num_fn = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == False:\n",
    "            num_fn += 1\n",
    "    return num_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_and_recall(data, annotations):\n",
    "    tp,fp,fn = true_positives(data, annotations), false_positives(data, annotations), false_negatives(data, annotations)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no web access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  8\n",
      "Num unsures:  675\n",
      "Accuracy: 0.5720\n",
      "Precision: 0.5448\n",
      "Recall: 0.8640\n",
      "Accuracy without unsures: 0.7515\n"
     ]
    }
   ],
   "source": [
    "no_web_file = \"../results/results_no_web_access.json\"\n",
    "result_data = get_results(no_web_file)\n",
    "result_data = result_data[:1000]\n",
    "num_disagreements(result_data)\n",
    "precision, recall = get_precision_and_recall(result_data, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data, test_data_annotations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with web access (only when model unsure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  16\n",
      "Num unsures:  329\n",
      "Accuracy: 0.5960\n",
      "Precision: 0.5804\n",
      "Recall: 0.6860\n",
      "Accuracy without unsures: 0.6433\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_web_access_no_initial_context.json\"\n",
    "result_data2 = get_results(web_access_file)\n",
    "num_disagreements(result_data2)\n",
    "precision, recall = get_precision_and_recall(result_data2, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data2, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data2, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  7\n",
      "Num unsures:  9\n",
      "Accuracy: 0.8580\n",
      "Precision: 0.8266\n",
      "Recall: 0.9060\n",
      "Accuracy without unsures: 0.8587\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_initial_context.json\"\n",
    "result_data3 = get_results(web_access_file)\n",
    "num_disagreements(result_data3)\n",
    "precision, recall = get_precision_and_recall(result_data3, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data3, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data3, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing where model fails (what kinds of examples the model fails on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils.data import get_data, show_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_summary(key):\n",
    "    with open(\"../scripts/final_summaries.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_samples(incorrect_idx, res_data):\n",
    "    for i in incorrect_idx:\n",
    "        img, caption, _, annotation = get_data(i)\n",
    "        display(img)\n",
    "        show_data(i)\n",
    "        key = str(annotation['id'])+\"_\"+str(annotation['image_id'])\n",
    "        print(\"Associated summary: \", retrieve_summary(key))\n",
    "        print(\"Model_prediction: \", res_data[i]['falsified'])\n",
    "        print(\"Model arguments: \", res_data[i]['output'])\n",
    "        cont = input()\n",
    "        if \"exit\" == cont:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unsures:  8\n",
      "[3, 4, 14, 17, 19, 31, 36, 38, 44, 47, 53, 62, 64, 67, 74, 76, 78, 85, 88, 110, 112, 113, 116, 117, 120, 132, 141, 146, 149, 150, 152, 154, 164, 167, 170, 172, 174, 175, 183, 190, 199, 201, 203, 205, 207, 216, 218, 223, 224, 226, 229, 234, 235, 237, 241, 242, 245, 253, 254, 256, 260, 267, 272, 274, 278, 286, 295, 296, 304, 305, 312, 316, 321, 322, 324, 332, 338, 339, 341, 342, 343, 344, 348, 352, 354, 367, 369, 371, 376, 384, 394, 398, 400, 404, 405, 414, 415, 418, 424, 425, 426, 427, 428, 432, 438, 451, 453, 454, 455, 456, 464, 466, 470, 480, 481, 484, 488, 493, 496, 502, 503, 505, 510, 513, 517, 518, 528, 529, 531, 532, 533, 535, 540, 541, 551, 556, 559, 566, 568, 580, 582, 593, 596, 600, 602, 608, 613, 614, 623, 626, 628, 636, 638, 640, 644, 651, 657, 661, 669, 672, 678, 689, 694, 698, 700, 708, 710, 712, 716, 719, 722, 724, 726, 728, 732, 736, 738, 740, 757, 759, 762, 763, 768, 772, 774, 784, 786, 787, 788, 790, 794, 811, 814, 819, 823, 824, 826, 827, 833, 835, 843, 847, 853, 856, 859, 874, 879, 883, 887, 888, 890, 891, 895, 896, 897, 914, 918, 926, 927, 930, 942, 944, 951, 954, 957, 962, 971, 972, 988, 995]\n"
     ]
    }
   ],
   "source": [
    "incorrect_idx = get_incorrect_idx(result_data3, test_data_annotations)\n",
    "print(incorrect_idx)\n",
    "#failed_samples(incorrect_idx, result_data3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
