{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    #print(\"Number of data points: \", len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_annotations():\n",
    "    visual_news_data = json.load(open(\"../../datasets/visualnews/origin/data.json\"))\n",
    "    visual_news_data_mapping = {ann[\"id\"]: ann for ann in visual_news_data}\n",
    "\n",
    "    test_data = json.load(open(\"../../news_clippings/news_clippings/data/merged_balanced/test.json\"))\n",
    "    annotations = test_data[\"annotations\"]\n",
    "    return annotations\n",
    "test_data_annotations = get_test_data_annotations()\n",
    "test_data_annotations = test_data_annotations[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unsures(data):\n",
    "    \"\"\"\n",
    "    In instances where one of the models does not have any output, consider the output of the other model to be the truth\n",
    "    \"\"\"\n",
    "    num_unsures = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            num_unsures += 1\n",
    "            if data[i]['output']['model_0'] == \"\" and data[i]['output']['model_1'] != \"\":\n",
    "                if \"YES\" in data[i]['output']['model_1'] or \"Yes\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_1'] or \"No\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = False\n",
    "            elif data[i]['output']['model_0'] != \"\" and data[i]['output']['model_1'] == \"\":\n",
    "                if \"YES\" in data[i]['output']['model_0'] or \"Yes\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_0'] or \"No\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = False\n",
    "    print(\"Num unsures: \", num_unsures)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_disagreements(data):\n",
    "    \"\"\"\n",
    "    Function to check when models actually disagree and when they are just unsure\n",
    "    \"\"\"\n",
    "    disagreements = 0\n",
    "    for i in range(len(data)):\n",
    "        m0 = data[i]['output']['model_0']\n",
    "        m1 = data[i]['output']['model_1']\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            #check if models disagree\n",
    "            if (\"YES\" in m0 or \"Yes\" in m0) and (\"NO\" in m1 or \"No\" in m1 or \"Unsure\" in m1):\n",
    "                disagreements += 1\n",
    "            elif (\"NO\" in m0 or \"No\" in m0) and (\"YES\" in m1 or \"Yes\" in m1 or \"Unsure\" in m1):\n",
    "                disagreements += 1\n",
    "            elif \"Unsure\" in m0 and (\"YES\" in m1 or \"NO\" in m1 or \"Yes\" in m1 or \"No\" in m1):\n",
    "                disagreements += 1\n",
    "    print(\"Num disagreements: \", disagreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, annotations):\n",
    "    num_correct = 0\n",
    "    data = fix_unsures(data)\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            incorrect_idx.append(i)\n",
    "    print(\"Num incorrects: \", len(incorrect_idx))\n",
    "    return num_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_incorrect_idx(data, annotations):\n",
    "    num_correct = 0\n",
    "    data = fix_unsures(data)\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) != annotations[i]['falsified']:\n",
    "            incorrect_idx.append(i)\n",
    "    return incorrect_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_without_unsures(data, annotations):\n",
    "    num_correct, num_unsures = 0,0\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['falsified'] == 'Unsure':\n",
    "            num_unsures += 1\n",
    "        elif bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "    return num_correct/(len(data) - num_unsures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(data, annotations):\n",
    "    num_tp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == True:\n",
    "            num_tp += 1\n",
    "    return num_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positives(data, annotations):\n",
    "    num_fp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == False and bool(data[i]['falsified']) == True:\n",
    "            num_fp += 1\n",
    "    return num_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negatives(data, annotations):\n",
    "    num_fn = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == False:\n",
    "            num_fn += 1\n",
    "    return num_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_and_recall(data, annotations):\n",
    "    tp,fp,fn = true_positives(data, annotations), false_positives(data, annotations), false_negatives(data, annotations)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no web access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  4\n",
      "Num unsures:  675\n",
      "Num incorrects:  428\n",
      "Accuracy: 0.5720\n",
      "Precision: 0.5448\n",
      "Recall: 0.8640\n",
      "Accuracy without unsures: 0.7515\n"
     ]
    }
   ],
   "source": [
    "no_web_file = \"../results/results_no_web_access.json\"\n",
    "result_data = get_results(no_web_file)\n",
    "result_data = result_data[:1000]\n",
    "num_disagreements(result_data)\n",
    "precision, recall = get_precision_and_recall(result_data, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data, test_data_annotations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with web access (only when model unsure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  16\n",
      "Num unsures:  329\n",
      "Accuracy: 0.5960\n",
      "Precision: 0.5804\n",
      "Recall: 0.6860\n",
      "Accuracy without unsures: 0.6433\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_web_access_no_initial_context.json\"\n",
    "result_data2 = get_results(web_access_file)\n",
    "num_disagreements(result_data2)\n",
    "precision, recall = get_precision_and_recall(result_data2, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data2, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data2, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  7\n",
      "Num unsures:  9\n",
      "Accuracy: 0.8580\n",
      "Precision: 0.8266\n",
      "Recall: 0.9060\n",
      "Accuracy without unsures: 0.8587\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_initial_context.json\"\n",
    "result_data3 = get_results(web_access_file)\n",
    "num_disagreements(result_data3)\n",
    "precision, recall = get_precision_and_recall(result_data3, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data3, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data3, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLaVA not human baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  39\n",
      "Num unsures:  48\n",
      "Num incorrects:  247\n",
      "Accuracy: 0.7530\n",
      "Precision: 0.6841\n",
      "Recall: 0.9400\n",
      "Accuracy without unsures: 0.7712\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../../datasets/results/llava_not_human.json\"\n",
    "result_data3 = get_results(web_access_file)\n",
    "num_disagreements(result_data3)\n",
    "precision, recall = get_precision_and_recall(result_data3, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data3, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data3, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with disambiguation queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  5\n",
      "Num unsures:  12\n",
      "Accuracy: 0.7730\n",
      "Precision: 0.7468\n",
      "Recall: 0.8260\n",
      "Accuracy without unsures: 0.7786\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_with_disambiguation.json\"\n",
    "result_data4 = get_results(web_access_file)\n",
    "num_disagreements(result_data4)\n",
    "precision, recall = get_precision_and_recall(result_data4, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data4, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data4, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with opposite stances and disambiguation queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_access_file = \"../scripts/temp_final_disamb.json\"\n",
    "result_data5 = get_results(web_access_file)\n",
    "num_disagreements(result_data5)\n",
    "precision, recall = get_precision_and_recall(result_data5, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data5, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data5, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253\n",
      "Num disagreements:  1\n",
      "Num unsures:  6\n",
      "Accuracy: 0.7352\n",
      "Precision: 0.7153\n",
      "Recall: 0.7778\n",
      "Accuracy without unsures: 0.7368\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../scripts/disamb_res_updated_1.json\"\n",
    "result_data5 = get_results(web_access_file)\n",
    "print(len(result_data5))\n",
    "num_disagreements(result_data5)\n",
    "precision, recall = get_precision_and_recall(result_data5, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data5, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data5, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actor-skeptic setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Precision: 0.6610\n",
      "Recall: 0.6940\n",
      "Accuracy without unsures: 0.6945\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_actor_skeptic.json\"\n",
    "result_data6 = get_results(web_access_file)\n",
    "print(len(result_data6))\n",
    "#num_disagreements(result_data6)\n",
    "precision, recall = get_precision_and_recall(result_data6, test_data_annotations)\n",
    "#print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data6, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data6, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuned model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Num disagreements:  8\n",
      "Num unsures:  25\n",
      "Num incorrects:  182\n",
      "Accuracy: 0.8180\n",
      "Precision: 0.7449\n",
      "Recall: 0.9580\n",
      "Accuracy without unsures: 0.8192\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_finetuned.json\"\n",
    "result_data7 = get_results(web_access_file)\n",
    "print(len(result_data7))\n",
    "num_disagreements(result_data7)\n",
    "precision, recall = get_precision_and_recall(result_data7, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data7, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data7, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Num disagreements:  40\n",
      "Num unsures:  234\n",
      "Num incorrects:  451\n",
      "Accuracy: 0.5490\n",
      "Precision: 0.5180\n",
      "Recall: 0.9500\n",
      "Accuracy without unsures: 0.5912\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/results_finetuned_debate_refined.json\"\n",
    "result_data7 = get_results(web_access_file)\n",
    "print(len(result_data7))\n",
    "num_disagreements(result_data7)\n",
    "precision, recall = get_precision_and_recall(result_data7, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data7, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data7, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debating with judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, annotations):\n",
    "    num_correct = 0\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            incorrect_idx.append(i)\n",
    "    print(\"Num incorrects: \", len(incorrect_idx))\n",
    "    return num_correct/len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "Num unsures:  0\n",
      "Num incorrects:  9\n",
      "Accuracy: 0.6667\n",
      "Precision: 0.6667\n",
      "Recall: 0.6154\n",
      "Accuracy without unsures: 0.6667\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../scripts/dj_temp.json\"\n",
    "result_data7 = get_results(web_access_file)\n",
    "print(len(result_data7))\n",
    "#num_disagreements(result_data7)\n",
    "precision, recall = get_precision_and_recall(result_data7, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data7, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data7, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "Num unsures:  2\n",
      "Num incorrects:  20\n",
      "Accuracy: 0.9074\n",
      "Precision: 0.8729\n",
      "Recall: 0.9537\n",
      "Accuracy without unsures: 0.9159\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../../datasets/results/gpt4o_res_temp_final.json\"\n",
    "result_data7 = get_results(web_access_file)\n",
    "print(len(result_data7))\n",
    "#num_disagreements(result_data7)\n",
    "precision, recall = get_precision_and_recall(result_data7, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data7, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data7, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Num unsures:  0\n",
      "Num incorrects:  141\n",
      "Accuracy: 0.8590\n",
      "Precision: 0.7928\n",
      "Recall: 0.9720\n",
      "Accuracy without unsures: 0.8590\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../../datasets/results/gpt4o_final/final_compiled.json\"\n",
    "result_data7 = get_results(web_access_file)\n",
    "print(len(result_data7))\n",
    "#num_disagreements(result_data7)\n",
    "precision, recall = get_precision_and_recall(result_data7, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data7, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data7, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Num unsures:  17\n",
      "Num incorrects:  92\n",
      "Accuracy: 0.9080\n",
      "Precision: 0.8505\n",
      "Recall: 0.9900\n",
      "Accuracy without unsures: 0.9227\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../../datasets/results/gpt4o_final/corrected_results.json\"\n",
    "result_data7 = get_results(web_access_file)\n",
    "print(len(result_data7))\n",
    "#num_disagreements(result_data7)\n",
    "precision, recall = get_precision_and_recall(result_data7, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data7, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n",
    "print(\"Accuracy without unsures: {:.4f}\".format(get_acc_without_unsures(result_data7, test_data_annotations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing where model fails (what kinds of examples the model fails on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils.data import get_data, show_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_summary(key):\n",
    "    with open(\"../utils/summaries.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def failed_samples(incorrect_idx, res_data):\n",
    "    for i in incorrect_idx:\n",
    "        img, caption, _, annotation = get_data(i)\n",
    "        display(img)\n",
    "        show_data(i)\n",
    "        key = str(annotation['id'])+\"_\"+str(annotation['image_id'])\n",
    "        print(\"Associated summary: \", retrieve_summary(key))\n",
    "        print(\"Model_prediction: \", res_data[i]['falsified'])\n",
    "        print(\"Model arguments: \", res_data[i]['output'])\n",
    "        cont = input()\n",
    "        if \"exit\" == cont:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unsures:  0\n",
      "[10, 14, 31, 34, 38, 62, 74, 80, 110, 120, 126, 151, 152, 164, 175, 199, 205, 216, 218, 224, 226, 234, 241, 242, 246, 250, 256, 260, 272, 284, 286, 300, 304, 320, 334, 336, 338, 348, 354, 360, 362, 372, 384, 386, 389, 398, 400, 404, 406, 410, 412, 416, 428, 436, 438, 446, 460, 464, 466, 470, 473, 480, 482, 496, 498, 502, 510, 516, 518, 520, 528, 530, 535, 538, 554, 562, 566, 580, 584, 588, 598, 600, 602, 608, 614, 623, 626, 651, 660, 666, 668, 672, 676, 678, 684, 686, 700, 708, 719, 720, 722, 726, 728, 752, 760, 762, 770, 772, 774, 778, 780, 786, 790, 794, 806, 814, 823, 826, 832, 836, 838, 842, 854, 856, 860, 864, 872, 874, 882, 888, 890, 900, 932, 940, 952, 962, 965, 972, 974, 978, 990]\n"
     ]
    }
   ],
   "source": [
    "incorrect_idx = get_incorrect_idx(result_data7, test_data_annotations)\n",
    "print(incorrect_idx)\n",
    "#failed_samples(incorrect_idx, result_data7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incorrect_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
