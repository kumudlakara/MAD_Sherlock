{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    #print(\"Number of data points: \", len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_annotations():\n",
    "    visual_news_data = json.load(open(\"../../datasets/visualnews/origin/data.json\"))\n",
    "    visual_news_data_mapping = {ann[\"id\"]: ann for ann in visual_news_data}\n",
    "\n",
    "    test_data = json.load(open(\"../../news_clippings/news_clippings/data/merged_balanced/test.json\"))\n",
    "    annotations = test_data[\"annotations\"]\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_unsures(data):\n",
    "    \"\"\"\n",
    "    In instances where one of the models does not have any output, consider the output of the other model to be the truth\n",
    "    \"\"\"\n",
    "    num_unsures = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            num_unsures += 1\n",
    "            if data[i]['output']['model_0'] == \"\" and data[i]['output']['model_1'] != \"\":\n",
    "                if \"YES\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_1']:\n",
    "                    data[i]['falsified'] = False\n",
    "            elif data[i]['output']['model_0'] != \"\" and data[i]['output']['model_1'] == \"\":\n",
    "                if \"YES\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = True\n",
    "                elif \"NO\" in data[i]['output']['model_0']:\n",
    "                    data[i]['falsified'] = False\n",
    "    print(\"Num unsures: \", num_unsures)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_disagreements(data):\n",
    "    \"\"\"\n",
    "    Function to check when models actually disagree and when they are just unsure\n",
    "    \"\"\"\n",
    "    disagreements = 0\n",
    "    for i in range(len(data)):\n",
    "        m0 = data[i]['output']['model_0']\n",
    "        m1 = data[i]['output']['model_1']\n",
    "        if data[i]['falsified'] == \"Unsure\":\n",
    "            #check if models disagree\n",
    "            if \"YES\" in m0 and (\"NO\" in m1 or \"UNSURE\" in m1):\n",
    "                disagreements += 1\n",
    "            elif \"NO\" in m0 and (\"YES\" in m1 or \"UNSURE\" in m1):\n",
    "                disagreements += 1\n",
    "            elif \"UNSURE\" in m0 and (\"YES\" in m1 or \"NO\" in m1):\n",
    "                disagreements += 1\n",
    "    print(\"Num disagreements: \", disagreements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(data, annotations):\n",
    "    num_correct = 0\n",
    "    data = fix_unsures(data)\n",
    "    incorrect_idx = []\n",
    "    for i in range(len(data)):\n",
    "        if bool(data[i]['falsified']) == annotations[i]['falsified']:\n",
    "            num_correct += 1\n",
    "        else:\n",
    "            incorrect_idx.append(i)\n",
    "    return num_correct/len(data), incorrect_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positives(data, annotations):\n",
    "    num_tp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == True:\n",
    "            num_tp += 1\n",
    "    return num_tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positives(data, annotations):\n",
    "    num_fp = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == False and bool(data[i]['falsified']) == True:\n",
    "            num_fp += 1\n",
    "    return num_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negatives(data, annotations):\n",
    "    num_fn = 0\n",
    "    for i in range(len(data)):\n",
    "        if annotations[i]['falsified'] == True and bool(data[i]['falsified']) == False:\n",
    "            num_fn += 1\n",
    "    return num_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_and_recall(data, annotations):\n",
    "    tp,fp,fn = true_positives(data, annotations), false_positives(data, annotations), false_negatives(data, annotations)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no web access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  8\n",
      "Num unsures:  675\n",
      "Accuracy: 0.5720\n",
      "Precision: 0.5448\n",
      "Recall: 0.8640\n"
     ]
    }
   ],
   "source": [
    "no_web_file = \"../scripts/results3_no_web_access_less_unsure.json\"\n",
    "result_data = get_results(no_web_file)\n",
    "result_data = result_data[:1000]\n",
    "num_disagreements(result_data)\n",
    "test_data_annotations = get_test_data_annotations()\n",
    "precision, recall = get_precision_and_recall(result_data, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with web access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num disagreements:  10\n",
      "Num unsures:  202\n",
      "Accuracy: 0.5983\n",
      "Precision: 0.5826\n",
      "Recall: 0.6933\n"
     ]
    }
   ],
   "source": [
    "web_access_file = \"../results/final_result.json\"\n",
    "result_data2 = get_results(web_access_file)\n",
    "num_disagreements(result_data2)\n",
    "precision, recall = get_precision_and_recall(result_data2, test_data_annotations)\n",
    "print(\"Accuracy: {:.4f}\".format(get_accuracy(result_data2, test_data_annotations)))\n",
    "print(\"Precision: {:.4f}\".format(precision))\n",
    "print(\"Recall: {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing where model fails (what kinds of examples the model fails on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from utils.data import get_data, show_data\n",
    "\n",
    "def failed_samples(incorrect_idx, res_data):\n",
    "    for i in incorrect_idx:\n",
    "        img, caption, _, _ = get_data(i)\n",
    "        display(img)\n",
    "        show_data(i)\n",
    "        print(\"Model_prediction: \", res_data[i]['falsified'])\n",
    "        print(\"Model arguments: \", res_data[i]['output'])\n",
    "        cont = input()\n",
    "        if \"exit\" == cont:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, incorrect_idx = get_accuracy(result_data2, test_data_annotations)\n",
    "failed_samples(incorrect_idx, result_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
